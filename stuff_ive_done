Questions:
> How can we deal with interstellar extinction best?
> An apparent magnitude prior seems like a good shout/maybe the NN will work this out


Stuff I could do:
> Save a copy of the graph of the network on exit and save the validation data results to the class/elsewhere
> Add MAP value estimation to MixtureDensityNetwork and also add some plotting tools to show distributions at a specific data point (will be very useful for redshifts)
> Add priors to the loss functions
> Deal with interstellar extinction better, somehow
> Implement TensorBoard support

---- ---- ---- ---- ----

06/12/18
- THE PERTURBATION COEFFICIENT IS THE HERO I NEEDED
- MDNs are designed inherently to predict *generative* data models. Therefore, the training data must itself be generative.
- This has produced the best results yet! It actually slightly lowers the NMAD (probably a bit like a form of regularisation) and is getting results that really aren't far from EAZY. In addition, the 5 sigma outlier level is just *better.* This method causes more than half as few outliers, which is AWESOME!
- I should try using a perturbation function. It seems loosely analogous to the NMAD, so logically, each zspec will want to use a slightly different amount of perturbation (probably, for instance, as a function of increasing zspec.)

05/12/18
- Still lots of issues with the CDFs, even when normalising them to the allowed range. It just still doesn't really play ball and is a bit unstable.

04/12/18
- Ratio normalised column means are the new winner! Thankyou Stijn <3
- This network is a fucking cheat
- By putting distributions all over the place, including outside the redshift range, it can minimise the Wittman condition still
- Is it really a pdf if the area inside the allowed region doesn't sum to unity? << THERE'S YOUR PROBLEM
- Binary activation function is sorta helpful, but not massively
- YOU COULD NORMALISE THE CDFS AGAINST THE AREA ACTUALLY WITHIN THE ALLOWED RANGE!!! FUCK YES this would work :D

03/12/18
- Lots of testing and trying to tune the CDF loss, now with PDFs normalised with an estimate of their maximums

~~ Info on CDF loss latest testing round ~~
- Log cosh loss everywhere seems to be good for stability and the like
- The cdf test is a bit unstable and I would love if it was better
- Test 11 seems to be the settings winner so far (NMAD ~0.04)


---- Week X ----


30/11/18
- Added convolutional layers in the beginning of a fun time
- Began re-investigating the CDF lossfunc

~~ Info on different conv tests ~~
- Tanh activation seems better than relu still
- Conv layers offer a *marginal* improvement (that might just be from longer runtime tbh...)
- Test 6 (filter size 8, stride 4) worked the best! It beat its control by 15%.
- Hilariously, normal distributions are better still! (See test 11) yet without a convolutional layer, are a decent bit (25%) worse than beta distributions.

~~ Info on CDF tests ~~
- CDF lossfunc seems to vaaaguely help
- But ultimately, it becomes degenerate with a reduction in MAP accuracy before being helpful enough to improve PDF validity (test 17)
- Test 18 & 19 refactored to have the multiplier inside the log, but this seems to be a bad idea?! Doesn't seem to actually give enough onus on learning (duuuuhhh, since alog(b) == log(b^a) but log(a*b) == log(a) + log(b), aka the second one is just an offset)


29/11/18
~~ Info on different tuning tests ~~
- Removing outliers is a bad idea
- Linear interpolation between points doesn't help as much as you might hope - it presumably removes too many features
- Removing nan values from the mean seemed to make the column means estimate... worse?! (adds more outliers though)
- Normalised column means don't help at all vs column means it seems
- Robust scaling is indeedy a bit shite, and causes horizontal lines
- Not removing any columns at all is a Good Idea
- Normalised column mean has a marginally higher NMAD but marginally fewer outliers - test 14
- Log fluxes improves things
- Running without any errors at all is better... test 23
- Best missing data handling in order is:
	1. Row-normalised column means
	2. Linear interpolation
	3. Column means
	4. Row means
	5. Removing galaxies with missing data

---- Week IX ----

--------------------------


02/11/18
- Added some pdf plotting features, still need some bugfixing though
- Hit some mad bugs with nans appearing for no apparent reason
- Fixed temporarily with 64 bit precision, but this needs fixing long-term (probs helped by TensorBoard) in a better, less computationally intensive way

01/11/18
- Had a bit of a sad. It's ok though :)

31/10/18
- Running successfully on blog example stuff (yay!)
- Now using a couple of data scaling systems from sklearn, which are implemented in MixtureDensityNetwork (this needs better testing, and a way to spec scaling options on the minmax scaler)
- Looked into how to use TensorBoard. It looks like it'd be really awesome to add for diagnostics!

30/10/18
- MAP value calculator is made
- Looked at some ways of working out good hidden layer sizes
- Got a bit stressed about life =(
- Couldn't see how to add HDI and ETI error bound estimation without a total faff. Needs more faffing with to un-faff the situation.

29/10/18
- Began work on a MAP value calculator.
- Getting very tempted to add priors

---- Week V ----


26/10/18
- Changed loss functions to now use a class system (lol)
- Added regularisation to MixtureDensityNetwork
- MixtureDensityNetwork.validate() now returns output in a dictionary, with keys that are loss-function specific
- Regularisation improves the code on the test example
- The MDN code I have is now very ready for action, yay!

25/10/18
- Finished the barebones and testing of the MDN class!
- Added some gorgeous running helping (mostly modified from BATDOG), including ETAs and reporting only every few seconds
- This class is a total fucking babe because you can easily change parameters in the network like number of layers, size etc
- Some basic analysis has been added, like a loss function evolution plotter. Could do with adding MAP estimation.
- Also it only supports one lossfunc type (with no regularisation) atm and that sucks lol

24/10/18
- Started work on an MDN class to make the code a lot nicer

23/10/18
- Got an MDN example working!!! YAY (maybe)
- I had bigtime issues with tf.subtract() broadcasting not working. I *hope* that the cheeky fix I've done works.
- I'm not currently reproducing loss functions as low as in the Edward example, so fingers crossed it's even actually working.

22/10/18
- Got the blog example running on my computer
- Started investigating how MDNs work in tensorflow
- Found quite a few useful packages but struggled to get inter-compatibility
- Decided to plow on with using only tensorflow. It will be a bit harder but will save a hell of a lot of compatibility issues.
- Started on an example mdn!

---- Week IV ----


- had the fucking presentation

---- Week III

10/10/18
- Fuck BUCS lol, moving my shit!!1!
- Fixed the above
- Finished reading Brammer. Lots of useful stuff has come from reading it methinks.
- Started reading Quadri


09/10/18
- Worked on IMI poster again (u lazy aha)
- 

08/10/18
- More of Brammer
- Worked on my IMI poster instead

---- Week II ----


05/10/18
- Sorted Npairs vs deltaZ
- Now also overplotting a Gaussian
- Started reading Brammer, Quadri

04/10/18
- Done the zphot vs zspec plot
- Almost there on Npairs vs deltaZ, just need to make sure no invalid redshifts get added

03/10/18
- Read upto chapter 4 of Raschke

02/10/18
- XDrive mounted
- Started reading some Raschke stuff

01/10/18
- Got some of the ssh stuff working

---- Week I ----
